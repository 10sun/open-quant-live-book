[
["index.html", "The Open Quant Live Book Preface", " The Open Quant Live Book Thársis T. P. Souza 2019-02-17 Preface Description The book aims to be an Open Source introductory reference of the most important aspects of financial data analysis, algo trading, portfolio selection, econophysics and machine learning in finance with an emphasis in reproducibility and openness not to be found in most other typical Wall Street-like references. The Book is Open and we are looking for co-authors. Feel free to reach out or simply create a pull request with your contribution on our Github project. Working Contents The Basics I/O Stylized Facts Correlation &amp; Causation Algo Trading Investment Process Backtesting Factor Investing Limit Order Portfolio Optimization Modern Portfolio Theory Measuring Risk Linear Programming Machine Learning Intro Agent-Based Models Binary Classifiers AutoML Hierarchical Risk Parity Econophysics Entropy, Efficiency and Coupling Transfer Entropy, Information Transfer and Causality Financial Networks Alternative Data Book’s information First published at: openquants.com. Licensed under Attribution-NonCommercial-ShareAlike 4.0 International. Copyright (c) 2018. Thársis T. P. Souza. New York, NY. "],
["io.html", "Chapter 1 I/O 1.1 Importing Data 1.2 Data Sources 1.3 Conclusion", " Chapter 1 I/O In this Chapter, we will introduce basic functions to read text, excel and JSON files as well as large files. We will also show how to obtain free financial and economic data including End-of-day and real-time pricing; Company financials; Macroeconomic data. Data sources include The SEC; Quandl; IEX; Alpha Vantage. 1.1 Importing Data 1.1.1 Text Files The most basic and commonly used option to import data from text files in R is the use of the function read.table from the r-base. We can use this function to read text files with extensions such as .txt and .csv. dat.table &lt;- read.table(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- read.csv(file = &quot;&lt;name of your file&gt;.csv&quot;) The package readr provides functions for reading text data into R that are much faster that the functions from the r-base. The read_table function from the package readr provides a near-replacement for the read.table function. library(readr) dat.table &lt;- readr::read_table2(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- readr::read_csv(file = &quot;&lt;name of your file&gt;.csv&quot;) Another option to save data is to write it in rds format. Data stored in rds format has the advantage to keep the original data struture and type of the object saved. Also, .rds files are compressed and consume less space than files saved in .csv format. A data.frame object can be saved in rds format and then loaded back as follows: write_rds(dat.frame, path = &quot;&lt;name of your file&gt;.rds&quot;) dat.frame &lt;- read_rds(path = &quot;&lt;name of your file&gt;.rds&quot;) 1.1.2 Excel Files The package readxl has an ease to use interface to functions that load excel documents in R. The functions read_xls and read_xlsx can be used to read excel files as follows: library(readxl) readxl::read_xls(path = &quot;&lt;name of your file&gt;.xls&quot;) readxl::read_xlsx(path = &quot;&lt;name of your file&gt;.xlsx&quot;) The function read_excel() automatically detects the extension of the input file as follows: readxl::read_excel(&quot;&lt;name and extension of your file&gt;&quot;, sheet = &quot;&lt;sheet name or index&gt;&quot;) In the read_excel function, the sheet argument can receive either the target sheet name or index number, where sheet indexing starts at 1. The readxl has been oberving increased use compared to other comparable packages such as gdata and the xlsx due to its relative ease of use and performance. Also, the readxl do not have depency with external code libraries while the packages gdata and xlsx depend on ActiveState PERL and the Java JDK, respectively. 1.1.3 JSON Files JSON files are particularly used for transmitting data in web applications but also frequently used as a standard data interchange format. The jsonline package can be used to parse files in JSON format as follows: library(jsonlite) result_json &lt;- read_json(&quot;&lt;json file&gt;&quot;) 1.1.4 Large Files Fast data manipulation in a short and flexible syntax. 1.2 Data Sources In this section, we will show how to obtain financial and economic data from public sources. 1.2.1 Alpha Vantage Alpha Vantage offers free access to pricing data including: Stock Time Series Data; Physical and Digital/Crypto Currencies (e.g., Bitcoin); Technical Indicators and Sector Performances. The data are available in JSON and CSV formats via REST APIs. The quantmod and the alphavantager R packages offer a lightweight R interface to the Alpha Vantage API. Daily stock prices can be obtained with the quantmod::getSymbols function as follows: getSymbols(Symbols=&#39;AAPL&#39;, src=&quot;av&quot;, output.size=&quot;full&quot;, adjusted=TRUE, api.key=&#39;your API key&#39;) The output data is stored in an object with the same name as the corresponding symbol, in this example AAPL. The output data looks like the following AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted 13.6 16.2 13.5 16.2 6411700 0.510 16.5 16.6 15.2 15.9 5820300 0.499 15.9 20.0 14.8 18.9 16182800 0.595 18.8 19.0 17.3 17.5 9300200 0.550 17.4 18.6 16.9 18.2 6910900 0.571 18.1 19.4 17.5 18.2 7915600 0.571 We called the quantmod::getSymbols function with the following arguments: Symbols='AAPL' defines a character vector specifying the names of each symbol to be loaded, here specified by the symbol of the company Apple Inc.; src=&quot;av&quot; specifies the sourcing method, here defined with the value corresponding to Alpha Vantage; output.size=&quot;full&quot;specified length of the time series returned. The strings compact and full are accepted with the following specifications: compact returns only the latest 100 data points; full returns the full-length time series of up to 20 years of historical data; adjusted=TRUE defines a boolean variable to include a column of closing prices adjusted for dividends and splits; api.key specifies your Alpha Vantage API key. 1.2.2 IEX The IEX Group operates the Investors Exchange (IEX), a stock exchange for U.S. equities that is built for investors and companies. IEX offers U.S. reference and market data including end-of-day and intraday pricing data. IEX offers an API with “a set of services designed for developers and engineers. It can be used to build high-quality apps and services”. Data sourced from the IEX API is freely available for commercial subject to conditions and the use of their API is subject to additional terms of use. IEX lists the following github project as an unofficial API for R: https://github.com/imanuelcostigan/iex. We will provide examples on how to obtain intraday pricing data using this package. First, we will use the devtools to install the package directly from its github repository as follows: library(devtools) install_github(&quot;imanuelcostigan/iex&quot;) The iex package provides 4 set of functions as follows: last: Provides IEX near real time last sale price, size and time. Last is ideal for developers that need a lightweight stock quote. IEX API real time API documentation. market: Provides exchange trade volume data in near real time. IEX market API documentation. stats: A set of functions that return trading statistics. IEX stats API documentation. tops: Provides IEX’s aggregated bid and offer position in near real time for all securities on IEX’s displayed limit order book. IEX API TOPS documentation. For instance, the last function has the following arguments: symbols: A vector of tickers (case insensitive). Special characters will be escaped. A list of eligible symbols is published daily by the IEX. When set to NULL (default) returns values for all symbols. fields: A vector of fields names to return (case sensitive). When set to NULL (default) returns values for all fields. version: The API version number, which is used to define the API URL. We can obtain intraday stock price data with the last function as follows: dat &lt;- iex::last(symbols = c(&quot;AAPL&quot;), fields = c(&quot;symbol&quot;, &quot;price&quot;, &quot;size&quot;)) The function returns an S3 object of class iex_api which has three accessible fields: path , response and content. The path contains the corresponding IEX API path: dat$path ## [1] &quot;tops/last&quot; The response contains the unparsed IEX API response: dat$response ## Response [https://api.iextrading.com/1.0/tops/last?symbols=AAPL&amp;filter=symbol%2Cprice%2Csize] ## Date: 2019-02-17 23:16 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 45 B The content contains the parsed content from the API’s response: dat$content ## [[1]] ## [[1]]$symbol ## [1] &quot;AAPL&quot; ## ## [[1]]$price ## [1] 142 ## ## [[1]]$size ## [1] 100 According to the developer, this package causes R to pause 0.2 seconds after executing an API call to avoid the user being throttled by the IEX API (which enforces a 5 request per second limit). Documentation about the other set of functions can be obtained at https://github.com/imanuelcostigan/iex/tree/master/man. 1.2.3 Quandl 1.2.4 SEC Official filings are freely available from the U.S. Securities and Exchange Commission’s EDGAR database. The package finreportr provides an interface in R to facilitate financial analysis from SEC’s 10K and 10K/A filings. We can obtain company basic information with the function the CompanyInfo function by passing the ticker symbol of the target company as follows: library(&quot;finreportr&quot;) AAPL.Info&lt;-CompanyInfo(&quot;AAPL&quot;) print(AAPL.Info) ## company CIK SIC state state.inc FY.end street.address ## 1 APPLE INC 0000320193 3571 CA CA 0930 ONE APPLE PARK WAY ## city.state ## 1 CUPERTINO CA 95014 As a result, we obtain the following information: Company name: APPLE INC; SEC Central Index Key (CIK): 0000320193; Standard Industrial Classification (SIC): 3571, which is the industry code for Electronic Computers; Address: ONE APPLE PARK WAY, CUPERTINO CA 95014; Most recent period of report end is 0930. The list of company annual reports with corresponding filing dates can be obtained with the function AnnualReports as follows: AAPL.reports&lt;-AnnualReports(&quot;AAPL&quot;) Table 1.1: Sample Annual Reports filing.name filing.date accession.no 10-K 2018-11-05 0000320193-18-000145 10-K 2017-11-03 0000320193-17-000070 10-K 2016-10-26 0001628280-16-020309 10-K 2015-10-28 0001193125-15-356351 10-K 2014-10-27 0001193125-14-383437 10-K 2013-10-30 0001193125-13-416534 The accession number is a unique identifier that the SEC creates for each filing. Company financials are organized into 3 segments: Income Statement, Balance Sheet and Cash Flow. Income Statement Financials from the Income Statement segment can be obtained with the GetIncome function as follows: AAPL.IS&lt;-GetIncome(&quot;AAPL&quot;, 2017) Table 1.2: Sample Income Statement Financials Metric Units Amount startDate endDate Revenue, Net usd 233715000000 2014-09-28 2015-09-26 Revenue, Net usd 75872000000 2015-09-27 2015-12-26 Revenue, Net usd 50557000000 2015-12-27 2016-03-26 Revenue, Net usd 42358000000 2016-03-27 2016-06-25 Revenue, Net usd 46852000000 2016-06-26 2016-09-24 Revenue, Net usd 215639000000 2015-09-27 2016-09-24 The Income Statement function returns data for the following metrics: Table 1.3: Income Statement Metrics Metrics Revenue, Net Cost of Goods and Services Sold Gross Profit Research and Development Expense Selling, General and Administrative Expense Operating Expenses Operating Income (Loss) Nonoperating Income (Expense) Income (Loss) from Continuing Operations before Income Taxes, Noncontrolling Interest Income Tax Expense (Benefit) Net Income (Loss) Attributable to Parent Earnings Per Share, Basic Earnings Per Share, Diluted Weighted Average Number of Shares Outstanding, Basic Weighted Average Number of Shares Outstanding, Diluted Common Stock, Dividends, Per Share, Declared Balance Sheet Financials from the Balance Sheet segment can be obtained with the GetBalanceSheet function as follows: AAPL.BS&lt;-GetBalanceSheet(&quot;AAPL&quot;, 2017) Table 1.4: Sample Balance Sheet Financials Metric Units Amount startDate endDate Cash and Cash Equivalents, at Carrying Value usd 13844000000 NA 2014-09-27 Cash and Cash Equivalents, at Carrying Value usd 21120000000 NA 2015-09-26 Cash and Cash Equivalents, at Carrying Value usd 20484000000 NA 2016-09-24 Cash and Cash Equivalents, at Carrying Value usd 20289000000 NA 2017-09-30 Available-for-sale Securities, Current usd 46671000000 NA 2016-09-24 Available-for-sale Securities, Current usd 53892000000 NA 2017-09-30 The Balance Sheet function returns data for the following metrics: Table 1.5: Balance Sheet Metrics Metrics Cash and Cash Equivalents, at Carrying Value Available-for-sale Securities, Current Accounts Receivable, Net, Current Inventory, Net Nontrade Receivables, Current Other Assets, Current Assets, Current Available-for-sale Securities, Noncurrent Property, Plant and Equipment, Net Goodwill Intangible Assets, Net (Excluding Goodwill) Other Assets, Noncurrent Assets Accounts Payable, Current Accrued Liabilities, Current Deferred Revenue, Current Commercial Paper Long-term Debt, Current Maturities Liabilities, Current Deferred Revenue, Noncurrent Long-term Debt, Excluding Current Maturities Other Liabilities, Noncurrent Liabilities Commitments and Contingencies Common Stocks, Including Additional Paid in Capital Retained Earnings (Accumulated Deficit) Accumulated Other Comprehensive Income (Loss), Net of Tax Stockholders’ Equity Attributable to Parent Liabilities and Equity Cash Flow Financials from the Cash Flow segment can be obtained with the GetCashFlow function as follows: AAPL.CF&lt;-GetCashFlow(&quot;AAPL&quot;, 2017) Table 1.6: Sample Cash Flow Financials Metric Units Amount startDate endDate Cash and Cash Equivalents, at Carrying Value usd 13844000000 NA 2014-09-27 Cash and Cash Equivalents, at Carrying Value usd 21120000000 NA 2015-09-26 Cash and Cash Equivalents, at Carrying Value usd 20484000000 NA 2016-09-24 Cash and Cash Equivalents, at Carrying Value usd 20289000000 NA 2017-09-30 Net Income (Loss) Attributable to Parent usd 53394000000 2014-09-28 2015-09-26 Net Income (Loss) Attributable to Parent usd 18361000000 2015-09-27 2015-12-26 The Cash Flow function returns data for the following metrics: Table 1.7: Cash Flow Metrics Metrics Cash and Cash Equivalents, at Carrying Value Net Income (Loss) Attributable to Parent Depreciation, Amortization and Accretion, Net Share-based Compensation Deferred Income Tax Expense (Benefit) Other Noncash Income (Expense) Increase (Decrease) in Accounts Receivable Increase (Decrease) in Inventories Increase (Decrease) in Other Receivables Increase (Decrease) in Other Operating Assets Increase (Decrease) in Accounts Payable Increase (Decrease) in Deferred Revenue Increase (Decrease) in Other Operating Liabilities Net Cash Provided by (Used in) Operating Activities Payments to Acquire Available-for-sale Securities Proceeds from Maturities, Prepayments and Calls of Available-for-sale Securities Proceeds from Sale of Available-for-sale Securities Payments to Acquire Businesses, Net of Cash Acquired Payments to Acquire Property, Plant, and Equipment Payments to Acquire Intangible Assets Payments to Acquire Other Investments Payments for (Proceeds from) Other Investing Activities Net Cash Provided by (Used in) Investing Activities Proceeds from Issuance of Common Stock Excess Tax Benefit from Share-based Compensation, Financing Activities Payments Related to Tax Withholding for Share-based Compensation Payments of Dividends Payments for Repurchase of Common Stock Proceeds from Issuance of Long-term Debt Repayments of Long-term Debt Proceeds from (Repayments of) Commercial Paper Net Cash Provided by (Used in) Financing Activities Cash and Cash Equivalents, Period Increase (Decrease) Income Taxes Paid, Net Interest Paid 1.3 Conclusion We showed how to load and import data from both local files and external sources. We provided examples on how to read tabular data and how to handle large files. We showed how to obtain financial and economic data from freely available sources. 1.3.1 Further Reading To further learn how to use R to load, transform, visualize and model data see (Wickham and Grolemund 2017). Additional relevant R packages include: dplyr: Fast data frames manipulation and database query. reshape2: Flexibly rearrange, reshape and aggregate data. readr: A fast and friendly way to read tabular data into R. tidyr: Easily tidy data with spread and gather functions. rlist: A toolbox for non-tabular data manipulation with lists. jsonlite: A robust and quick way to parse JSON files in R. ff: Data structures designed to store large datasets. lubridate: A set of functions to work with dates and times. References "],
["stylized-facts.html", "Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.3 Volatility 2.4 Correlation", " Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.2.1 Fat Tails 2.2.2 Skewness 2.3 Volatility 2.3.1 Time-invariance 2.3.2 Volatility Clustering 2.3.3 Correlation with Trading Volume 2.4 Correlation 2.4.1 Time-invariance 2.4.2 Auto-correlation "],
["correlation-causation.html", "Chapter 3 Correlation &amp; Causation 3.1 Introduction 3.2 A First Definition of Causality 3.3 Quantifying Granger-Causality 3.4 Conclusion", " Chapter 3 Correlation &amp; Causation 3.1 Introduction 3.2 A First Definition of Causality We quantify causality by using the notion of the causal relation introduced by Granger (Wiener 1956; Granger 1969) where a signal \\(X\\) is said to Granger-cause \\(Y\\) if the future realizations of \\(Y\\) can be better explained using the past information from \\(X\\) and \\(Y\\) rather than \\(Y\\) alone. The most common definitions of Granger-causality rely on the prediction of a future value of the variable \\(Y\\) by using the past values of \\(X\\) and \\(Y\\) itself. In that form, \\(X\\) is said to G-cause \\(Y\\) if the use of \\(X\\) improves the prediction of \\(Y\\). Let \\(X_t\\) be a random variable associated at time \\(t\\) while \\(X^t\\) represents the collection of random variables up to time \\(t\\). We consider \\({X_t}, {Y_t}\\) and \\({Z_t}\\) to be three stochastic processes. Let \\(\\hat Y_{t+1}\\) be a predictor for the value of the variable \\(Y\\) at time \\(t+1\\). We compare the expected value of a loss function \\(g(e)\\) with the error \\(e=\\hat{Y}_{t+1} - Y_{t+1}\\) of two models: The expected value of the prediction error given only \\(Y^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_1(X^{t},Z^t))] \\end{equation}\\] The expected value of the prediction error given \\(Y^t\\) and \\(X^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, X^{t},Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_2(X^{t},Y^t,Z^t))]. \\end{equation}\\] In both models, the functions \\(f_1(.)\\) and \\(f_2(.)\\) are chosen to minimize the expected value of the loss function. In most cases, these functions are retrieved with linear and, possibly, with nonlinear regressions. Typical forms for \\(g(.)\\) are the \\(l1\\)- or \\(l2\\)-norms. We can now provide our first definition of statistical causality under the Granger causal notion as follows: Definition 3.1 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(\\mathcal{R}(Y_{t+1} \\; | \\; X^t, Y^t, Z^t) = \\mathcal{R}(Y_{t+1} \\; | \\; Y^t, Z^t)\\). A more general definition than Def. 3.1 that does not depend on assuming prediction functions can be formulated by considering conditional probabilities. A probabilistic definition of G-causality assumes that \\(Y_{t+1}\\) and \\(X^{t}\\) are independent given the past information \\((X^{t}, Y^{t})\\) if and only if \\(p(Y_{t+1} \\, | \\, X^{t}, Y^{t}, Z^{t}) = p(Y_{t+1} \\, | \\, Y^{t}, Z^{t})\\), where \\(p(. \\, | \\, .)\\) represents the conditional probability distribution. In other words, omitting past information from \\(X\\) does not change the probability distribution of \\(Y\\). This leads to our second definition of statistical causality as follows: Definition 3.2 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(Y_{t+1} {\\perp\\!\\!\\!\\!\\perp}X^{t} \\; | \\; Y^{t}, Z^{t}\\). Def. 3.2 does not assume any functional form in the coupling between \\(X\\) and \\(Y\\). Nevertheless, it requires a method to assess their conditional dependency. In the next Section, we define a parametric linear specification of G-causality based on Def. 3.1. Later in the book, in the Section 6.2, when we cover Econophysics techniques, we will present a nonlinear specification for G-causality based on Def. 3.2. 3.3 Quantifying Granger-Causality We will take the following procedure to quantify Granger-causality according to Def. 3.1: Specify two predictive models: The first considers \\(Y^t\\) to predict \\(Y^{t+1}\\) (Model \\(\\mathcal{M}\\)); The second considers \\(Y^t\\) and \\(X^t\\) to predict \\(Y^{t+1}\\) (Model \\(\\mathcal{M}^*\\)); Test for model misspecification; Test the hypothesis that the expected value of the prediction error of the Models \\(\\mathcal{M}\\) and \\(\\mathcal{M}^*\\) are statistically the same; Apply correction for multiple hypothesis testing. If the null hypothesis from 3. is rejected then there is evidence that \\(X\\) Granger-causes \\(Y\\) under Def. 3.1. 3.3.1 Model Specification Standard Granger-causality tests assume a linear relationship among the causes and effects and are implemented by fitting autoregressive models (Wiener 1956; Granger 1969). Consider the linear vector-autoregressive (VAR) equations: \\[\\begin{align} Y(t) &amp;= {\\alpha} + \\sum^k_{\\Delta t=1}{{\\beta}_{\\Delta t} Y(t-\\Delta t)} + \\epsilon_t, \\tag{3.1}\\\\ Y(t) &amp;= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} Y(t-\\Delta t)} + \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}X(t-\\Delta t)}+ \\widehat{\\epsilon}_t, \\tag{3.2} \\end{align}\\] where \\(k\\) is the number of lags considered. From Def 3.1, \\(X\\) does not G-cause \\(Y\\) if and only if the prediction errors of \\(X\\) in the restricted Eq. (3.1) and unrestricted regression models Eq. (3.2) are equal (i.e., they are statistically indistinguishable). 3.3.2 Test for Misspecification A statistically significant causality can be reported only if the linear models from Eqs. (3.1) and (3.2) are not misspecified. For that purpose, we utilize the BDS test (Brock et al. 1996) for the model misspecification. The BDS test (Brock et al. 1996) is used to detect nonlinear dependence in time series. When applied to the residuals of a linear model, the BDS tests the null hypothesis that these residuals are independent and identically distributed. The BDS test is a powerful test to detect linear misspecification and nonlinearity (Brock et al. 1996; W. A. Barnett et al. 1997). Let \\(\\epsilon_t = (\\epsilon_{t=1}, \\ldots, \\epsilon_{t=n})\\) be the residuals of the linear fitted model and define its \\(m\\)-embedding as \\(\\epsilon_t^m = (\\epsilon_{t}, \\epsilon_{t-1}, \\ldots, \\epsilon_{t-m+1})\\). The \\(m\\)-embedding correlation integral is given by \\[\\begin{align} C_{m,n}(\\Delta \\epsilon) = \\frac{2}{k(k-1)}\\sum_{s = 1}^{t}{\\sum_{t=s}^{n}{ \\chi(\\| \\epsilon_s^m - \\epsilon_t^m \\|, \\Delta \\epsilon) }}, \\nonumber \\end{align}\\] and \\[\\begin{align} C_{m}(\\Delta \\epsilon) = \\lim_{n\\to\\infty} C_{m,n}(\\Delta \\epsilon), \\nonumber \\end{align}\\] where \\(\\chi\\) is an indicator function where \\(\\chi(\\| \\epsilon_s^m - \\epsilon_t^m \\|, \\Delta \\epsilon) = 1\\) if \\(\\| \\epsilon_s^m - \\epsilon_t^m \\| &lt; \\Delta \\epsilon\\) and zero, otherwise. The null hypothesis of the BDS test assumes that \\(\\epsilon_t\\) is iid. In this case, \\[\\begin{align} C_{m}(\\Delta \\epsilon) = C_{1}(\\Delta \\epsilon)^m. \\nonumber \\end{align}\\] The BDS statistic is a measure of the extent that this relation holds in the data. This statistic is given by the following: \\[\\begin{align} V_{m}(\\Delta \\epsilon) = \\sqrt{n}\\frac{C_{m}(\\Delta \\epsilon) - C_{1}(\\Delta \\epsilon)^m}{\\sigma_m(\\Delta \\epsilon)}, \\nonumber \\end{align}\\] where \\(\\sigma_m(\\Delta \\epsilon)\\) can be estimated as described in (Brock et al. 1996). The null hypothesis of the BDS test indicates that the model tested is not misspecified and it is rejected at the 5% significance level if \\(\\|V_m(\\Delta \\epsilon)\\| &gt; 1.96\\). The parameter \\(\\Delta \\epsilon\\) is commonly set as a factor of the variance (\\(\\sigma_\\epsilon\\)) of \\(\\epsilon\\). 3.3.3 Analysis of Variance A one-way ANOVA test is utilized to test if the residuals from Eqs. (3.1) and (3.2) differ from each other significantly. 3.3.4 Multiple Hypotheses Testing Correction When more than one lag \\(k\\) is tested, a Bonferroni correction is applied to control for multiple hypotheses testing. 3.4 Conclusion References "],
["limit-order.html", "Chapter 4 Limit Order", " Chapter 4 Limit Order "],
["entropy.html", "Chapter 5 Entropy 5.1 Efficiency and Bubbles in the Crypto and Equity Markets 5.2 Quantifying Non-linear Correlation Between Equity and Commodity Markets", " Chapter 5 Entropy Let \\(X\\) be a random variable and \\(P_X(x)\\) be its probability density function (pdf). The entropy \\(H(X)\\) is a measure of the uncertainty of \\(X\\) and is defined in the discrete case as follows: \\[\\begin{equation} H(X) = -\\sum_{x \\in X}{P_X(x)\\log{P_X(x)}}. \\label{eq:H} \\end{equation}\\] If the \\(\\log\\) is taken to base two, then the unit of \\(H\\) is the (binary digit). We employ the natural logarithm which implies the unit in (natural unit of information). Given a coupled system \\((X,Y)\\), where \\(P_Y(y)\\) is the pdf of the random variable \\(Y\\) and \\(P_{X,Y}\\) is the joint pdf between \\(X\\) and \\(Y\\), the joint entropy between \\(X\\) and \\(Y\\) is given by the following: \\[\\begin{equation} H(X,Y) = -\\sum_{x \\in X}{\\sum_{y \\in Y}{P_{X,Y}(x,y)\\log{P_{X,Y}(x,y)}}}. \\label{eq:HXY} \\end{equation}\\] The conditional entropy is defined by the following: \\[\\begin{equation} H\\left(Y\\middle\\vert X\\right) = H(X,Y) - H(X). \\end{equation}\\] We can interpret \\(H\\left(Y\\middle\\vert X\\right)\\) as the uncertainty of \\(Y\\) given a realization of \\(X\\). 5.1 Efficiency and Bubbles in the Crypto and Equity Markets 5.2 Quantifying Non-linear Correlation Between Equity and Commodity Markets "],
["transfer-entropy.html", "Chapter 6 Transfer Entropy 6.1 Introduction 6.2 Nonlinear G-Causality 6.3 The Link Between Linear Granger-causality and Transfer Entropy 6.4 Net Information Flow 6.5 Empirical Experiment: Information Flow on Simulated Systems 6.6 Empirical Experiment: Information Flow on Global Markets", " Chapter 6 Transfer Entropy 6.1 Introduction 6.2 Nonlinear G-Causality To compute the nonlinear G-Causality, we use the concept of Transfer Entropy. Since its introduction (Schreiber 2000), Transfer Entropy has been recognized as an important tool in the analysis of causal relationships in nonlinear systems (Hlavackovaschindler et al. 2007). It detects directional and dynamical information (Montalto 2014) while not assuming any particular functional form to describe interactions among systems. The Transfer Entropy can be defined as the difference between the conditional entropies: \\[\\begin{equation} TE\\left(X \\rightarrow Y\\right \\vert Z) = H\\left(Y^F\\middle\\vert Y^P,Z^P\\right) - H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right), \\tag{6.1} \\end{equation}\\] which can be rewritten as a sum of Shannon entropies: \\[\\begin{align} TE\\left(X \\rightarrow Y\\right) = H\\left(Y^P, X^P\\right) - H\\left(Y^F, Y^P, X^P\\right) + H\\left(Y^F, Y^P\\right) - H\\left(Y^P\\right), \\end{align}\\] where \\(Y^F\\) is a forward time-shifted version of \\(Y\\) at lag \\(\\Delta t\\) relatively to the past time-series \\(X^P\\), \\(Y^P\\) and \\(Z^P\\). Within this framework we say that \\(X\\) does not G-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(H\\left(Y^F\\middle\\vert Y^P,Z^P \\right) = H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right)\\), i.e., when \\(TE\\left(X \\rightarrow Y,Z^P\\right) = 0\\). Empirically, we reject this null hypothesis of causality if the Transfer Entropy from \\(X\\) to \\(Y\\) is significantly higher than the shuffled version of the original data. For this we estimate 400 replicates of \\(TE(X_{Shuffled} \\rightarrow Y)\\), where \\(X_{Shuffled}\\) is a random permutation of \\(X\\) relatively to \\(Y\\). We compute the randomized Transfer Entropy at each permutation for each time-shift (\\(\\Delta t\\)) from 1 to 10 days. We then calculated the frequency at which the observed Transfer Entropy was equal or more extreme than the randomized Transfer Entropy. The statistical significance was assessed using p-value \\(&lt; 0.05\\) after Bonferroni correction. 6.3 The Link Between Linear Granger-causality and Transfer Entropy It has been shown (L. Barnett, Barrett, and Seth 2009) that linear G-causality and Transfer Entropy are equivalent if all processes are jointly Gaussian. In particular, by assuming the standard measure (\\(l2\\)-norm loss function) of linear G-causality for the bivariate case as follows (see Section 3.3 for more details on linear-Granger causality): \\[\\begin{equation} GC_{X \\rightarrow Y} = \\log\\left( \\frac{var(\\epsilon_t)}{var( \\widehat{\\epsilon}_t)} \\right), \\tag{6.2} \\end{equation}\\] the following can be proved (L. Barnett, Barrett, and Seth 2009): \\[\\begin{align} TE_{X \\rightarrow Y} = GC_{X \\rightarrow Y}/2. \\tag{6.3} \\end{align}\\] This result provides a direct mapping between the Transfer Entropy and the linear G-causality implemented in the standard VAR framework. Hence, it is possible to estimate the TE both in its general form and with its equivalent form for linear G-causality. 6.4 Net Information Flow Transfer-entropy is an asymmetric measure, i.e., \\(T_{X \\rightarrow Y} \\neq T_{Y \\rightarrow X}\\), and it thus allows the quantification of the directional coupling between systems. The Net Information Flow is defined as \\[\\begin{equation} \\widehat{TE}_{X \\rightarrow Y} = TE_{X \\rightarrow Y} - TE_{Y \\rightarrow X}\\;. \\end{equation}\\] One can interpret this quantity as a measure of the dominant direction of the information flow. In other words, a positive result indicates a dominant information flow from \\(X\\) to \\(Y\\) compared to the other direction or, similarly, it indicates which system provides more predictive information about the other system (Michalowicz, Nichols, and Bucholtz 2013). In the next sections we will provide empirical examples that show that Transfer Entropy can capture information flow in both linear and nonlinear systems. 6.5 Empirical Experiment: Information Flow on Simulated Systems In this section, we construct simulated systems and test the nonlinear and linear formulations of the net information flow. We show that only the nonlinear formulation of net information flow is able to capture the nonlinear relationships in the simulated systems. For the nonlinear case, we compute Transfer Entropy as defined in Eq. (6.1). Conversely, to estimate the linear version of the Net Information Flow, we computed the Transfer Entropy using Eq. (6.3), i.e., by estimating linear G-causality Eq. (6.2) under a linear-VAR framework. 6.6 Empirical Experiment: Information Flow on Global Markets References "],
["financial-networks.html", "Chapter 7 Financial Networks 7.1 Introduction 7.2 Network Construction 7.3 Applications", " Chapter 7 Financial Networks 7.1 Introduction Financial markets can be regarded as a complex network in which nodes represent different financial assets and edges represent one or many types of relationships among those assets. Filtered correlation-based networks have successfully been used in the literature to study financial markets structure particularly from observational data derived from empirical financial time series (Bardoscia et al. 2017; S. A. L. Tumminello Michele AND Miccichè 2011; R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010, M. Tumminello et al. (2005)). The underlying principle is to use correlations from empirical financial time series to construct a sparse network representing the most relevant connections. Analyses on filtered correlation-based networks for information extraction (Song, Aste, and Di Matteo 2008; T. Aste, Shaw, and Di Matteo 2010) have widely been used to explain market interconnectedness from high-dimensional data. Applications include asset allocation (Y. Li et al. 2018; Pozzi, Di Matteo, and Aste 2013), market stability assessments (Morales et al. 2012), hierarchical structure analyses (R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010; Musmeci, Aste, and Matteo 2014; Song, Di Matteo, and Aste 2012) and the identification of lead-lag relationships (Curme, Stanley, and Vodenska 2015). In this Chapter we will describe how to Construct and filter financial networks; Build price-based dynamic industry taxonomies; Implement a trading strategy based on financial network structure. 7.2 Network Construction We selected \\(N = 100\\) of the most capitalized companies that were part of the S&amp;P500 index from 09/05/2012 to 08/25/2017. The list of these companies’ ticker symbols is reported in the Appendix . For each stock \\(i\\) the financial variable was defined as the daily stock’s log-return \\(R_i(\\tau)\\) at time \\(\\tau\\). Stock returns \\(R_i\\) and social media opinion scores \\(O_i\\) each amounted to a time series of length equals to 1251 trading days. These series were divided time-wise into \\(M = 225\\) windows \\(t = 1, 2, \\ldots, M\\) of width \\(T = 126\\) trading days. A window step length parameter of \\(\\delta T = 5\\) trading days defined the displacement of the window, i.e., the number of trading days between two consecutive windows. The choice of window width \\(T\\) and window step \\(\\delta T\\) is arbitrary, and it is a trade-off between having analysis that is either too dynamic or too smooth. The smaller the window width and the larger the window steps, the more dynamic the data are. To characterize the synchronous time evolution of assets, we used equal time Kendall’s rank coefficients between assets \\(i\\) and \\(j\\), defined as \\[\\begin{equation} \\rho_{i, j}(t) = \\sum\\limits_{t&#39; &lt; \\tau}sgn(V_i(t&#39;) - V_i(\\tau))sgn(V_j(t&#39;) - V_j(\\tau)), \\end{equation}\\] where \\(t&#39;\\) and \\(\\tau\\) are time indexes within the window \\(t\\) and \\(V_i \\in \\{R_i, O_i\\}\\). Kendall’s rank coefficients takes into account possible nonlinear (monotonic) relationships. It fulfill the condition \\(-1 \\leq \\rho_{i, j} \\leq 1\\) and form the \\(N \\times N\\) correlation matrix \\(C(t)\\) that served as the basis for the networks constructed in this work. To construct the asset-based financial and social networks, we defined a distance between a pair of stocks. This distance was associated with the edge connecting the stocks, and it reflected the level at which they were correlated. We used a simple non-linear transformation \\(d_{i, j}(t) = \\sqrt{2(1 - \\rho_{i,j}(t))}\\) to obtain distances with the property \\(2 \\geq d_{i,j} \\geq 0\\), forming a \\(N \\times N\\) symmetric distance matrix \\(D(t)\\). 7.2.1 Network Filtering: Asset Graphs We extract the \\(N(N-1)/2\\) distinct distance elements from the upper triangular part of the distance matrix \\(D(t)\\), which were then sorted in an ascending order to form an ordered sequence \\(d_1(t), d_2(t), \\ldots, d_{N(N-1)/2}(t)\\). Since we require the graph to be representative of the market, it is natural to build the network by including only the strongest connections. This is a network filtering procedure that has been successfully applied in the construction of for the analyses of market structure . The number of edges to include is arbitrary, and we included those from the bottom quartile, which represented the 25% shortest edges in the graph (largest correlations), thus giving \\(E(t) = \\{d_1(t), d_2(t), \\ldots, d_{\\floor{N/4}}(t)\\}\\). We denoted \\(E^{F}(t)\\) as the set of edges constructed from the distance matrix derived from stock returns \\(R(t)\\). The financial network considered is \\(G^{F} = ( V, E^{F} )\\), where \\(V\\) is the vertex set of stocks. 7.2.2 Network Filtering: MST 7.2.3 Network Filtering: PMFG 7.3 Applications 7.3.1 Industry Taxonomy 7.3.2 Portfolio Construction References "],
["statistical-methods.html", "A Statistical Methods A.1 Kernel Density Estimation", " A Statistical Methods This Appendix provides details to some of statistical methods used in the book. A.1 Kernel Density Estimation In the entropy computation (see Section 5) the empirical probability distribution must be estimated. Histogram-based methods and kernel density estimations are the two main methods for that. Histogram-based is the simplest and most used nonparametric density estimator. Nonetheless, it yields density estimates that have discontinuities and vary significantly depending on the bin size choice. Also known as the Parzen-Rosenblatt window method, the kernel density estimation (KDE) approach approximates the density function at point \\(x\\) using neighboring observations. However, instead of building up the estimate according to the bin edges as in histograms, the KDE method uses each point of estimation \\(x\\) as the center of a bin of width \\(2h\\) and weight it according to a kernel function. Thereby, the kernel estimate of the probability density function \\(f(x)\\) is defined as \\[\\begin{equation} \\hat{f} = \\frac{1}{nh}\\sum_{x&#39; \\in X}{K\\left(\\frac{x - x&#39;}{h}\\right)}. \\tag{A.1} \\end{equation}\\] A usual choice for the kernel \\(K\\), which we use here, is the (Gaussian) radial basis function: \\[\\begin{equation} K(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}x^2}. \\end{equation}\\] The problem of selecting the bandwidth \\(h\\) in Eq. (A.1) is crucial in the density estimation. A large \\(h\\) will oversmooth the estimated density and mask the structure of the data. On the other hand, a small bandwidth will reduce the bias of the density estimate at the expense of a larger variance in the estimates. If we assume that the true distribution is Gaussian and we use a Gaussian kernel, the optimal value of \\(h\\) that minimizes the mean integrated squared error (MISE) is \\[\\begin{equation*} h^* = 1.06\\sigma N^{-1/5}, \\end{equation*}\\] where \\(N\\) is the total number of points and \\(\\sigma\\) can be estimated as the sample standard deviation. This bandwidth estimation is often called the Gaussian approximation or Silverman’s rule of thumb for kernel density estimation (Silverman and Green 1986). This is the most commonly-used method and it is here employed. Other common methods are given by (Sheather and Jones 1991) and (Scott 1992). References "],
["references.html", "References", " References "]
]
