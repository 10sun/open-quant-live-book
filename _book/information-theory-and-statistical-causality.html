<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 4 Information Theory and Statistical Causality | The Open Quant Live Book</title>
  <meta name="description" content="Chapter 4 Information Theory and Statistical Causality | The Open Quant Live Book">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 4 Information Theory and Statistical Causality | The Open Quant Live Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="./fig/cover.jpg" />
  <meta property="og:description" content="Chapter 4 Information Theory and Statistical Causality | The Open Quant Live Book" />
  <meta name="github-repo" content="souzatharsis/open-quant-live-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Information Theory and Statistical Causality | The Open Quant Live Book" />
  
  <meta name="twitter:description" content="Chapter 4 Information Theory and Statistical Causality | The Open Quant Live Book" />
  <meta name="twitter:image" content="./fig/cover.jpg" />

<meta name="author" content="OpenQuants.com">


<meta name="date" content="2019-08-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="entropy.html">
<link rel="next" href="financial-networks.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-75703863-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-75703863-2');
</script>


<!-- Hypotheses -->
<!-- <script src="https://hypothes.is/embed.js" async></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="custom_style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Open Quant Live Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#description"><i class="fa fa-check"></i>Description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contribute"><i class="fa fa-check"></i>Contribute</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#working-contents"><i class="fa fa-check"></i>Working Contents</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#books-information"><i class="fa fa-check"></i>Book’s information</a></li>
</ul></li>
<li class="part"><span><b>I The Basics</b></span></li>
<li class="chapter" data-level="1" data-path="io.html"><a href="io.html"><i class="fa fa-check"></i><b>1</b> I/O</a><ul>
<li class="chapter" data-level="1.1" data-path="io.html"><a href="io.html#importing-data"><i class="fa fa-check"></i><b>1.1</b> Importing Data</a><ul>
<li class="chapter" data-level="1.1.1" data-path="io.html"><a href="io.html#text-files"><i class="fa fa-check"></i><b>1.1.1</b> Text Files</a></li>
<li class="chapter" data-level="1.1.2" data-path="io.html"><a href="io.html#excel-files"><i class="fa fa-check"></i><b>1.1.2</b> Excel Files</a></li>
<li class="chapter" data-level="1.1.3" data-path="io.html"><a href="io.html#json-files"><i class="fa fa-check"></i><b>1.1.3</b> JSON Files</a></li>
<li class="chapter" data-level="1.1.4" data-path="io.html"><a href="io.html#large-files"><i class="fa fa-check"></i><b>1.1.4</b> Large Files</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="io.html"><a href="io.html#data-sources"><i class="fa fa-check"></i><b>1.2</b> Data Sources</a><ul>
<li class="chapter" data-level="1.2.1" data-path="io.html"><a href="io.html#alpha-vantage"><i class="fa fa-check"></i><b>1.2.1</b> Alpha Vantage</a></li>
<li class="chapter" data-level="1.2.2" data-path="io.html"><a href="io.html#iex"><i class="fa fa-check"></i><b>1.2.2</b> IEX</a></li>
<li class="chapter" data-level="1.2.3" data-path="io.html"><a href="io.html#quandl"><i class="fa fa-check"></i><b>1.2.3</b> Quandl</a></li>
<li class="chapter" data-level="1.2.4" data-path="io.html"><a href="io.html#sec"><i class="fa fa-check"></i><b>1.2.4</b> SEC</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="io.html"><a href="io.html#conclusion"><i class="fa fa-check"></i><b>1.3</b> Conclusion</a><ul>
<li class="chapter" data-level="1.3.1" data-path="io.html"><a href="io.html#further-reading"><i class="fa fa-check"></i><b>1.3.1</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="stylized-facts.html"><a href="stylized-facts.html"><i class="fa fa-check"></i><b>2</b> Stylized Facts</a><ul>
<li class="chapter" data-level="2.1" data-path="stylized-facts.html"><a href="stylized-facts.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="stylized-facts.html"><a href="stylized-facts.html#distribution-of-returns"><i class="fa fa-check"></i><b>2.2</b> Distribution of Returns</a><ul>
<li class="chapter" data-level="2.2.1" data-path="stylized-facts.html"><a href="stylized-facts.html#fat-tails"><i class="fa fa-check"></i><b>2.2.1</b> Fat Tails</a></li>
<li class="chapter" data-level="2.2.2" data-path="stylized-facts.html"><a href="stylized-facts.html#skewness"><i class="fa fa-check"></i><b>2.2.2</b> Skewness</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="stylized-facts.html"><a href="stylized-facts.html#volatility"><i class="fa fa-check"></i><b>2.3</b> Volatility</a><ul>
<li class="chapter" data-level="2.3.1" data-path="stylized-facts.html"><a href="stylized-facts.html#time-invariance"><i class="fa fa-check"></i><b>2.3.1</b> Time-invariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="stylized-facts.html"><a href="stylized-facts.html#volatility-clustering"><i class="fa fa-check"></i><b>2.3.2</b> Volatility Clustering</a></li>
<li class="chapter" data-level="2.3.3" data-path="stylized-facts.html"><a href="stylized-facts.html#correlation-with-trading-volume"><i class="fa fa-check"></i><b>2.3.3</b> Correlation with Trading Volume</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="stylized-facts.html"><a href="stylized-facts.html#correlation"><i class="fa fa-check"></i><b>2.4</b> Correlation</a><ul>
<li class="chapter" data-level="2.4.1" data-path="stylized-facts.html"><a href="stylized-facts.html#time-invariance-1"><i class="fa fa-check"></i><b>2.4.1</b> Time-invariance</a></li>
<li class="chapter" data-level="2.4.2" data-path="stylized-facts.html"><a href="stylized-facts.html#auto-correlation"><i class="fa fa-check"></i><b>2.4.2</b> Auto-correlation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Algo Trading</b></span></li>
<li class="part"><span><b>III Portfolio Optimization</b></span></li>
<li class="part"><span><b>IV Machine Learning</b></span></li>
<li class="part"><span><b>V Econophysics</b></span></li>
<li class="chapter" data-level="3" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>3</b> Entropy</a><ul>
<li class="chapter" data-level="3.1" data-path="entropy.html"><a href="entropy.html#definition"><i class="fa fa-check"></i><b>3.1</b> Definition</a></li>
<li class="chapter" data-level="3.2" data-path="entropy.html"><a href="entropy.html#efficiency-and-bubbles-a-case-study-in-the-crypto-and-equity-markets"><i class="fa fa-check"></i><b>3.2</b> Efficiency and Bubbles: A Case Study in the Crypto and Equity Markets</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html"><i class="fa fa-check"></i><b>4</b> Information Theory and Statistical Causality</a><ul>
<li class="chapter" data-level="4.1" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#LinearG"><i class="fa fa-check"></i><b>4.1</b> A First Definition of Causality</a></li>
<li class="chapter" data-level="4.2" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#a-probabilistic-based-definition"><i class="fa fa-check"></i><b>4.2</b> A Probabilistic-Based Definition</a></li>
<li class="chapter" data-level="4.3" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#nonlinearG"><i class="fa fa-check"></i><b>4.3</b> Transfer Entropy and Statistical Causality</a></li>
<li class="chapter" data-level="4.4" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#net-information-flow"><i class="fa fa-check"></i><b>4.4</b> Net Information Flow</a></li>
<li class="chapter" data-level="4.5" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#the-link-between-granger-causality-and-transfer-entropy"><i class="fa fa-check"></i><b>4.5</b> The Link Between Granger-causality and Transfer Entropy</a></li>
<li class="chapter" data-level="4.6" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#information-flow-on-simulated-systems"><i class="fa fa-check"></i><b>4.6</b> Information Flow on Simulated Systems</a></li>
<li class="chapter" data-level="4.7" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#information-flow-among-international-stock-market-indices"><i class="fa fa-check"></i><b>4.7</b> Information Flow among International Stock Market Indices</a></li>
<li class="chapter" data-level="4.8" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#other-applications"><i class="fa fa-check"></i><b>4.8</b> Other Applications</a><ul>
<li class="chapter" data-level="4.8.1" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#quantifying-information-flow-between-social-media-and-the-stock-market"><i class="fa fa-check"></i><b>4.8.1</b> Quantifying Information Flow Between Social Media and the Stock Market</a></li>
<li class="chapter" data-level="4.8.2" data-path="information-theory-and-statistical-causality.html"><a href="information-theory-and-statistical-causality.html#detecting-causal-links-between-investor-sentiment-and-cryptocurrency-prices"><i class="fa fa-check"></i><b>4.8.2</b> Detecting Causal Links Between Investor Sentiment and Cryptocurrency Prices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="financial-networks.html"><a href="financial-networks.html"><i class="fa fa-check"></i><b>5</b> Financial Networks</a><ul>
<li class="chapter" data-level="5.1" data-path="financial-networks.html"><a href="financial-networks.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="financial-networks.html"><a href="financial-networks.html#network-construction"><i class="fa fa-check"></i><b>5.2</b> Network Construction</a><ul>
<li class="chapter" data-level="5.2.1" data-path="financial-networks.html"><a href="financial-networks.html#network-filtering-asset-graphs"><i class="fa fa-check"></i><b>5.2.1</b> Network Filtering: Asset Graphs</a></li>
<li class="chapter" data-level="5.2.2" data-path="financial-networks.html"><a href="financial-networks.html#network-filtering-mst"><i class="fa fa-check"></i><b>5.2.2</b> Network Filtering: MST</a></li>
<li class="chapter" data-level="5.2.3" data-path="financial-networks.html"><a href="financial-networks.html#network-filtering-pmfg"><i class="fa fa-check"></i><b>5.2.3</b> Network Filtering: PMFG</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="financial-networks.html"><a href="financial-networks.html#applications"><i class="fa fa-check"></i><b>5.3</b> Applications</a><ul>
<li class="chapter" data-level="5.3.1" data-path="financial-networks.html"><a href="financial-networks.html#industry-taxonomy"><i class="fa fa-check"></i><b>5.3.1</b> Industry Taxonomy</a></li>
<li class="chapter" data-level="5.3.2" data-path="financial-networks.html"><a href="financial-networks.html#portfolio-construction"><i class="fa fa-check"></i><b>5.3.2</b> Portfolio Construction</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Alternative Data</b></span></li>
<li class="chapter" data-level="6" data-path="the-market-the-players-and-the-rules.html"><a href="the-market-the-players-and-the-rules.html"><i class="fa fa-check"></i><b>6</b> The Market, The Players and The Rules</a><ul>
<li class="chapter" data-level="6.1" data-path="the-market-the-players-and-the-rules.html"><a href="the-market-the-players-and-the-rules.html#the-market"><i class="fa fa-check"></i><b>6.1</b> The Market</a></li>
<li class="chapter" data-level="6.2" data-path="the-market-the-players-and-the-rules.html"><a href="the-market-the-players-and-the-rules.html#the-data"><i class="fa fa-check"></i><b>6.2</b> The Data</a></li>
<li class="chapter" data-level="6.3" data-path="the-market-the-players-and-the-rules.html"><a href="the-market-the-players-and-the-rules.html#the-buyers"><i class="fa fa-check"></i><b>6.3</b> The Buyers</a></li>
<li class="chapter" data-level="6.4" data-path="the-market-the-players-and-the-rules.html"><a href="the-market-the-players-and-the-rules.html#conclusion-1"><i class="fa fa-check"></i><b>6.4</b> Conclusion</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="statistical-methods.html"><a href="statistical-methods.html"><i class="fa fa-check"></i><b>A</b> Statistical Methods</a><ul>
<li class="chapter" data-level="A.1" data-path="statistical-methods.html"><a href="statistical-methods.html#kde"><i class="fa fa-check"></i><b>A.1</b> Kernel Density Estimation</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a><ul>
<li class="chapter" data-level="B.1" data-path="datasets.html"><a href="datasets.html#dt-indices"><i class="fa fa-check"></i><b>B.1</b> Log-Returns of International Stock Market Indices Prices</a><ul>
<li class="chapter" data-level="B.1.1" data-path="datasets.html"><a href="datasets.html#dataset-location"><i class="fa fa-check"></i><b>B.1.1</b> Dataset Location</a></li>
<li class="chapter" data-level="B.1.2" data-path="datasets.html"><a href="datasets.html#dataset-description"><i class="fa fa-check"></i><b>B.1.2</b> Dataset Description</a></li>
<li class="chapter" data-level="B.1.3" data-path="datasets.html"><a href="datasets.html#original-data-source"><i class="fa fa-check"></i><b>B.1.3</b> Original Data Source</a></li>
<li class="chapter" data-level="B.1.4" data-path="datasets.html"><a href="datasets.html#code"><i class="fa fa-check"></i><b>B.1.4</b> Code</a></li>
<li class="chapter" data-level="B.1.5" data-path="datasets.html"><a href="datasets.html#dataset-scheme"><i class="fa fa-check"></i><b>B.1.5</b> Dataset Scheme</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://creativecommons.org/licenses/by/4.0/" target="blank">CC-BY 4.0</a></li>
<li><a href="https://github.com/souzatharsis/open-quant-live-book" target="blank">See source code on Github</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Open Quant Live Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="information-theory-and-statistical-causality" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Information Theory and Statistical Causality</h1>
<p>We’ve all heard the say “correlation does not imply causation”, but how can we quantify causation? This is an extremely difficult and often misleading task, particularly trying to infer causality from observational data and cannot perform controlled trials or A/B testing.</p>
Take for example the 2-dimensional system from Fig. <a href="information-theory-and-statistical-causality.html#fig:rand">4.1</a>. At a first glance, one could say that there is no clear relationship or causality between the random variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. However, this apparent random system presents a causal relationship defined by the following simple equations:
<span class="math display">\[\begin{align*}
x_1 &amp;= 0.441x_1 + \epsilon_1 \\
x_2 &amp;= 0.51x_1^2 + \epsilon_2, \\ 
&amp;\epsilon_1, \epsilon_2 \sim \mathcal{N}(0,1).
\end{align*}\]</span>
<p>A simple nonlinearity introduced in the relationship between <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_1\)</span> was enough to introduce enough complexity into the system and potentially mislead a naive (non-quant) human.</p>
<div class="figure" style="text-align: center"><span id="fig:rand"></span>
<img src="chapters/TransferEntropy/rand.png" alt="Life is Random (or Nonlinear?)" width="50%" />
<p class="caption">
Figure 4.1: Life is Random (or Nonlinear?)
</p>
</div>
<p>Fortunately, we can take advantage of statistics and information theory to uncover complex causal relationships from observational data (remember, this is still a very very challenging task). We quantify causality by using the notion of the causal relation introduced by Granger <span class="citation">(Wiener <a href="#ref-Wiener56">1956</a>; Granger <a href="#ref-granger:econ">1969</a>)</span>, where a signal <span class="math inline">\(X\)</span> is said to Granger-cause <span class="math inline">\(Y\)</span> if the future realizations of <span class="math inline">\(Y\)</span> can be better explained using the past information from <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> rather than <span class="math inline">\(Y\)</span> alone.</p>
<p>In this Chapter, we will introduce a prediction-based definition of causality and its implementation using a vector auto-regression formulation. Next, we will introduce a probabilistic-definition of causality and its implementation using an information-theoretical framework. We will then symulate linear and nonlinear systems and uncover causal links with the proposed methods. In the final section, we will quantify information flow among global equity indexes further uncovering which indexes and markets are driving the global financial markets.</p>
<div id="LinearG" class="section level2">
<h2><span class="header-section-number">4.1</span> A First Definition of Causality</h2>
<p>The most common definitions of Granger-causality (<em>G-causality</em>) rely on the prediction of a future value of the variable <span class="math inline">\(Y\)</span> by using the past values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> itself. In that form, <span class="math inline">\(X\)</span> is said to <em>G-cause</em> <span class="math inline">\(Y\)</span> if the use of <span class="math inline">\(X\)</span> improves the prediction of <span class="math inline">\(Y\)</span>.</p>
<p>Let <span class="math inline">\(X_t\)</span> be a random variable associated at time <span class="math inline">\(t\)</span> while <span class="math inline">\(X^t\)</span> represents the collection of random variables up to time <span class="math inline">\(t\)</span>. We consider <span class="math inline">\({X_t}, {Y_t}\)</span> and <span class="math inline">\({Z_t}\)</span> to be three stochastic processes. Let <span class="math inline">\(\hat Y_{t+1}\)</span> be a predictor for the value of the variable <span class="math inline">\(Y\)</span> at time <span class="math inline">\(t+1\)</span>.</p>
<p>We compare the expected value of a loss function <span class="math inline">\(g(e)\)</span> with the error <span class="math inline">\(e=\hat{Y}_{t+1} - Y_{t+1}\)</span> of two models:</p>
<ol style="list-style-type: decimal">
<li>The expected value of the prediction error given only <span class="math inline">\(Y^t\)</span>
<span class="math display">\[\begin{equation}
 \mathcal{R}(Y^{t+1} \, | \, Y^t,Z^t) = \mathbb{E}[g(Y_{t+1} - f_1(X^{t},Z^t))]
\end{equation}\]</span></li>
<li>The expected value of the prediction error given <span class="math inline">\(Y^t\)</span> and <span class="math inline">\(X^t\)</span>
<span class="math display">\[\begin{equation}
 \mathcal{R}(Y^{t+1} \, | \, X^{t},Y^t,Z^t) = \mathbb{E}[g(Y_{t+1} - f_2(X^{t},Y^t,Z^t))].
\end{equation}\]</span></li>
</ol>
<p>In both models, the functions <span class="math inline">\(f_1(.)\)</span> and <span class="math inline">\(f_2(.)\)</span> are chosen to minimize the expected value of the loss function. In most cases, these functions are retrieved with linear and, possibly, with nonlinear regressions, neural networks etc. Typical forms for <span class="math inline">\(g(.)\)</span> are the <span class="math inline">\(l1\)</span>- or <span class="math inline">\(l2\)</span>-norms.</p>
<p>We can now provide our first definition of statistical causality under the Granger causal notion as follows:</p>

<div class="definition">
<span id="def:G1" class="definition"><strong>Definition 4.1  </strong></span><span class="math inline">\(X\)</span> does not Granger-cause <span class="math inline">\(Y\)</span> relative to side information <span class="math inline">\(Z\)</span> if and only if <span class="math inline">\(\mathcal{R}(Y_{t+1} \; | \; X^t, Y^t, Z^t) = \mathcal{R}(Y_{t+1} \; | \; Y^t, Z^t)\)</span>.
</div>
<p> <br /></p>
<p>Standard Granger-causality tests assume a functional form in the relationship among the causes and effects and are implemented by fitting autoregressive models <span class="citation">(Wiener <a href="#ref-Wiener56">1956</a>; Granger <a href="#ref-granger:econ">1969</a>)</span>.</p>
Consider the linear vector-autoregressive (VAR) equations:
<span class="math display" id="eq:AR22" id="eq:AR11">\[\begin{align}
Y(t) &amp;= {\alpha} + \sum^k_{\Delta t=1}{{\beta}_{\Delta t} Y(t-\Delta t)} + \epsilon_t, \tag{4.1}\\
Y(t) &amp;= \widehat{\alpha} + \sum^k_{\Delta t=1}{{\widehat{\beta}}_{\Delta t} Y(t-\Delta t)} +  \sum^k_{\Delta t=1}{{\widehat{\gamma}}_{\Delta t}X(t-\Delta t)}+ \widehat{\epsilon}_t, \tag{4.2}
\end{align}\]</span>
<p>where <span class="math inline">\(k\)</span> is the number of lags considered. Alternatively, you can choose your DL/SVM/RF/GLM method of choice to fit the model.</p>
<p>From Def <a href="information-theory-and-statistical-causality.html#def:G1">4.1</a>, <span class="math inline">\(X\)</span> does not G-cause <span class="math inline">\(Y\)</span> if and only if the prediction errors of <span class="math inline">\(X\)</span> in the restricted Eq. <a href="information-theory-and-statistical-causality.html#eq:AR11">(4.1)</a> and unrestricted regression models Eq. <a href="information-theory-and-statistical-causality.html#eq:AR22">(4.2)</a> are equal (i.e., they are statistically indistinguishable). A one-way ANOVA test can be utilized to test if the residuals from Eqs. <a href="information-theory-and-statistical-causality.html#eq:AR11">(4.1)</a> and <a href="information-theory-and-statistical-causality.html#eq:AR22">(4.2)</a> differ from each other significantly. When more than one lag <span class="math inline">\(k\)</span> is tested, a correction for multiple hypotheses testing should be applied, e.g. False Discovery Rate (FDR) or Bonferroni correction.</p>
</div>
<div id="a-probabilistic-based-definition" class="section level2">
<h2><span class="header-section-number">4.2</span> A Probabilistic-Based Definition</h2>
<p>A more general definition than Def. <a href="information-theory-and-statistical-causality.html#def:G1">4.1</a> that does not depend on assuming prediction functions can be formulated by considering conditional probabilities. A probabilistic definition of G-causality assumes that <span class="math inline">\(Y_{t+1}\)</span> and <span class="math inline">\(X^{t}\)</span> are independent given the past information <span class="math inline">\((X^{t}, Y^{t})\)</span> if and only if <span class="math inline">\(p(Y_{t+1} \, | \, X^{t}, Y^{t}, Z^{t}) = p(Y_{t+1} \, | \, Y^{t}, Z^{t})\)</span>, where <span class="math inline">\(p(. \, | \, .)\)</span> represents the conditional probability distribution. In other words, omitting past information from <span class="math inline">\(X\)</span> does not change the probability distribution of <span class="math inline">\(Y\)</span>. This leads to our second definition of statistical causality as follows:</p>

<div class="definition">
<span id="def:G2" class="definition"><strong>Definition 4.2  </strong></span><span class="math inline">\(X\)</span> does not Granger-cause <span class="math inline">\(Y\)</span> relative to side information <span class="math inline">\(Z\)</span> if and only if <span class="math inline">\(Y_{t+1} {\perp\!\!\!\!\perp}X^{t} \; | \; Y^{t}, Z^{t}\)</span>.
</div>
<p> <br /></p>
<p>Def. <a href="information-theory-and-statistical-causality.html#def:G2">4.2</a> does not assume any functional form in the coupling between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Nevertheless, it requires a method to assess their conditional dependency. In the next section, we will leverage an Information-Theoretical framework for that purpose.</p>
</div>
<div id="nonlinearG" class="section level2">
<h2><span class="header-section-number">4.3</span> Transfer Entropy and Statistical Causality</h2>
Given a coupled system <span class="math inline">\((X,Y)\)</span>, where <span class="math inline">\(P_Y(y)\)</span> is the pdf of the random variable <span class="math inline">\(Y\)</span> and <span class="math inline">\(P_{X,Y}\)</span> is the joint pdf between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the joint entropy between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by the following:
<span class="math display">\[\begin{equation}
H(X,Y) = -\sum_{x \in X}{\sum_{y \in Y}{P_{X,Y}(x,y)\log{P_{X,Y}(x,y)}}}.
\label{eq:HXY}
\end{equation}\]</span>
The conditional entropy is defined by the following:
<span class="math display">\[\begin{equation}
H\left(Y\middle\vert X\right) = H(X,Y) - H(X).
\end{equation}\]</span>
<p>We can interpret <span class="math inline">\(H\left(Y\middle\vert X\right)\)</span> as the uncertainty of <span class="math inline">\(Y\)</span> given a realization of <span class="math inline">\(X\)</span>.</p>
<p>To compute G-Causality, we use the concept of Transfer Entropy. Since its introduction <span class="citation">(Schreiber <a href="#ref-PhysRevLett.85.461">2000</a>)</span>, Transfer Entropy has been recognized as an important tool in the analysis of causal relationships in nonlinear systems <span class="citation">(Hlavackovaschindler et al. <a href="#ref-citeulike:1447442">2007</a>)</span>. It detects directional and dynamical information <span class="citation">(Montalto <a href="#ref-10.1371/journal.pone.0109462">2014</a>)</span> while not assuming any particular functional form to describe interactions among systems.</p>
The Transfer Entropy can be defined as the difference between the conditional entropies:
<span class="math display" id="eq:TE">\[\begin{equation}
 TE\left(X \rightarrow Y\right \vert Z) =  H\left(Y^F\middle\vert Y^P,Z^P\right) - H\left(Y^F\middle\vert X^P, Y^P,Z^P\right),
\tag{4.3}
\end{equation}\]</span>
which can be rewritten as a sum of Shannon entropies:
<span class="math display">\[\begin{align}
TE\left(X \rightarrow Y\right) = H\left(Y^P, X^P\right) - H\left(Y^F, Y^P, X^P\right) + H\left(Y^F, Y^P\right) - H\left(Y^P\right),
\end{align}\]</span>
<p>where <span class="math inline">\(Y^F\)</span> is a forward time-shifted version of <span class="math inline">\(Y\)</span> at lag <span class="math inline">\(\Delta t\)</span> relatively to the past time-series <span class="math inline">\(X^P\)</span>, <span class="math inline">\(Y^P\)</span> and <span class="math inline">\(Z^P\)</span>. Within this framework we say that <span class="math inline">\(X\)</span> does not G-cause <span class="math inline">\(Y\)</span> relative to side information <span class="math inline">\(Z\)</span> if and only if <span class="math inline">\(H\left(Y^F\middle\vert Y^P,Z^P \right) = H\left(Y^F\middle\vert X^P, Y^P,Z^P\right)\)</span>, i.e., when <span class="math inline">\(TE\left(X \rightarrow Y,Z^P\right) = 0\)</span>.</p>
<!-- Empirically, we reject this null hypothesis of causality if the Transfer Entropy from $X$ to $Y$ is significantly higher than the shuffled version of the original data. -->
<!-- For this we estimate 400 replicates of $TE(X_{Shuffled} \rightarrow Y)$, where $X_{Shuffled}$ is a random permutation of $X$ relatively to $Y$. -->
<!-- We compute the randomized Transfer Entropy at each permutation for each time-shift ($\Delta t$) from 1 to 10 days.  -->
<!-- We then calculated the frequency at which the observed Transfer Entropy was equal or more extreme  -->
<!-- than the randomized Transfer Entropy. The statistical significance was assessed using p-value $< 0.05$ after Bonferroni correction. -->
</div>
<div id="net-information-flow" class="section level2">
<h2><span class="header-section-number">4.4</span> Net Information Flow</h2>
Transfer-entropy is an asymmetric measure, i.e., <span class="math inline">\(T_{X \rightarrow Y} \neq T_{Y \rightarrow X}\)</span>, and it thus allows the quantification of the directional coupling between systems. The Net Information Flow is defined as
<span class="math display">\[\begin{equation}
\widehat{TE}_{X \rightarrow Y} = TE_{X \rightarrow Y} - TE_{Y \rightarrow X}\;.
\end{equation}\]</span>
<p>One can interpret this quantity as a measure of the dominant direction of the information flow. In other words, a positive result indicates a dominant information flow from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> compared to the other direction or, similarly, it indicates which system provides more predictive information about the other system <span class="citation">(Michalowicz, Nichols, and Bucholtz <a href="#ref-Michalowicz:2013:HDE:2601840">2013</a>)</span>.</p>
</div>
<div id="the-link-between-granger-causality-and-transfer-entropy" class="section level2">
<h2><span class="header-section-number">4.5</span> The Link Between Granger-causality and Transfer Entropy</h2>
<p>It has been shown <span class="citation">(Barnett, Barrett, and Seth <a href="#ref-PhysRevLett.103.238701">2009</a>)</span> that linear G-causality and Transfer Entropy are equivalent if all processes are jointly Gaussian. In particular, by assuming the standard measure (<span class="math inline">\(l2\)</span>-norm loss function) of linear G-causality for the bivariate case as follows (see Section <a href="information-theory-and-statistical-causality.html#LinearG">4.1</a> for more details on linear-Granger causality):</p>
<span class="math display" id="eq:GCGC">\[\begin{equation}
GC_{X \rightarrow Y} = \log\left( \frac{var(\epsilon_t)}{var( \widehat{\epsilon}_t)} \right),
\tag{4.4}
\end{equation}\]</span>
<p>the following can be proved <span class="citation">(Barnett, Barrett, and Seth <a href="#ref-PhysRevLett.103.238701">2009</a>)</span>:</p>
<span class="math display" id="eq:GCGC2">\[\begin{align}
TE_{X \rightarrow Y} = GC_{X \rightarrow Y}/2.
\tag{4.5}
\end{align}\]</span>
<p>This result provides a direct mapping between the Transfer Entropy and the linear G-causality implemented in the standard VAR framework. Hence, it is possible to estimate the TE both in its general form and with its equivalent form for linear G-causality.</p>
</div>
<div id="information-flow-on-simulated-systems" class="section level2">
<h2><span class="header-section-number">4.6</span> Information Flow on Simulated Systems</h2>
<p>In this section, we construct simulated systems to couple random variables in a causal manner. We then quantify information flow using the methods studied in this Chapter.</p>
We first assume a linear system, where random variables have linear relationships defined as follow:
<span class="math display">\[\begin{align}
x_1(n) &amp;= 0.95\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\ \nonumber
x_2(n) &amp;= 0.5x_1(n-1) + w_2\\ \nonumber
x_3(n) &amp;= -0.4x_1(n-1) + w_3\\ \nonumber
x_4(n) &amp;= -0.5x_1(n-1) + 0.25\sqrt{2}x_4(n-1) + 0.25\sqrt{2}x_5(n-1) + w_4\\ \nonumber
x_5(n) &amp;= -0.25\sqrt{2}x_4(n-1) + 0.25\sqrt{2}x_5(n-1) + w_5, \nonumber
\end{align}\]</span>
<p>where <span class="math inline">\(w_1, w_2, w_3, w_4, w_5 \sim N(0, 1)\)</span>. To simulate this system we assume <span class="math inline">\(x_i(0) = 0, i \in (1, 2, \ldots, 5)\)</span> as initial condition and then iteratively generate <span class="math inline">\(x_i\)</span> for <span class="math inline">\(n \in (1, 2, \ldots, N)\)</span> with a total of <span class="math inline">\(N = 2,000\)</span> iterations by randomly sampling <span class="math inline">\(w_i, i \in (1, 2, \ldots, 5)\)</span> from a normal distribution with zero mean and unit variance.</p>
<p>We can simulate this system with the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12345</span>)
n &lt;-<span class="st"> </span><span class="dv">2000</span>
x1 &lt;-<span class="st"> </span>x2&lt;-x3&lt;-x4&lt;-x5&lt;-<span class="kw">rep</span>(<span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)) {
  x1[i] &lt;-<span class="st"> </span><span class="fl">0.95</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>)<span class="op">*</span><span class="st"> </span>x1[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">-</span><span class="fl">0.9025</span><span class="op">*</span>x1[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)
  x2[i] &lt;-<span class="st"> </span><span class="fl">0.5</span><span class="op">*</span>x1[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)
  x3[i] &lt;-<span class="st"> </span><span class="op">-</span><span class="fl">0.4</span><span class="op">*</span>x1[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)
  x4[i] &lt;-<span class="st"> </span><span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x1[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="fl">0.25</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>)<span class="op">*</span>x4[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="fl">0.25</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>)<span class="op">*</span>x5[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)
  x5[i] &lt;-<span class="st"> </span><span class="op">-</span><span class="fl">0.25</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>)<span class="op">*</span>x4[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="fl">0.25</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>)<span class="op">*</span>x5[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)
}

x1 &lt;-<span class="st"> </span>x1[<span class="op">-</span><span class="dv">1</span>]
x2 &lt;-<span class="st"> </span>x2[<span class="op">-</span><span class="dv">1</span>]
x3 &lt;-<span class="st"> </span>x3[<span class="op">-</span><span class="dv">1</span>]
x4 &lt;-<span class="st"> </span>x4[<span class="op">-</span><span class="dv">1</span>]
x5 &lt;-<span class="st"> </span>x5[<span class="op">-</span><span class="dv">1</span>]
linear.system &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x1, x2, x3, x4, x5)</code></pre></div>
The Fig. <a href="information-theory-and-statistical-causality.html#fig:causality-graph1">4.2</a> A) represents the dependencies of the simulated linear system. The Fig. <a href="information-theory-and-statistical-causality.html#fig:causality-graph1">4.2</a> B) and Fig. <a href="information-theory-and-statistical-causality.html#fig:causality-graph1">4.2</a> C) show the linear and nonlinear Information Flows among the system’s variables, respectively. A cell <span class="math inline">\((x, y)\)</span> presents the information flow from variable <span class="math inline">\(y\)</span> to variable <span class="math inline">\(x\)</span>. From Fig. <a href="information-theory-and-statistical-causality.html#fig:causality-graph1">4.2</a>, we observe that both the linear and nonlinear approaches presented similar results, i.e., both methods captured the system’s dependencies similarly. This result is expected as the system is purely linear and the nonlinear information flow is able to capture both the linear and nonlinear interactions.
<div class="figure" style="text-align: center"><span id="fig:causality-graph1"></span>
<img src="chapters/TransferEntropy/S4Fig1.png" alt="Interactions between the variables of the simulated linear system." width="100%" />
<p class="caption">
Figure 4.2: Interactions between the variables of the simulated linear system.
</p>
</div>
<p>We define a second system by introducing nonlinear interactions between <span class="math inline">\(x_1\)</span> and the variables <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_5\)</span> as follows:</p>
<span class="math display">\[\begin{align}
x_1(n) &amp;= 0.95\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\ \nonumber
x_2(n) &amp;= 0.5x_1^2(n-1) + w_2\\ \nonumber
x_3(n) &amp;= -0.4x_1(n-1) + w_3\\ \nonumber
x_4(n) &amp;= -0.5x_1^2(n-1) + 0.25\sqrt{2}x_4(n-1) + 0.25\sqrt{2}x_5(n-1) + w_4\\ \nonumber
x_5(n) &amp;= -0.25\sqrt{2}x_4(n-1) + 0.25\sqrt{2}x_5(n-1) + w_5, \nonumber
\end{align}\]</span>
<p>where <span class="math inline">\(w_1, w_2, w_3, w_4\)</span> and <span class="math inline">\(w_5 \sim N(0, 1)\)</span>. To simulate this system we assume <span class="math inline">\(x_i(0) = 0, i \in (1, 2, ..., 5)\)</span> as initial condition and then iteratively generate <span class="math inline">\(x_i\)</span> for <span class="math inline">\(n \in (1, 2, ..., N)\)</span> with a total of <span class="math inline">\(N = 200,000\)</span> iterations by randomly sampling <span class="math inline">\(w_i, i \in (1, 2, ..., 5)\)</span> from a normal distribution with zero mean and unit variance.</p>
<p>The Fig. <a href="information-theory-and-statistical-causality.html#fig:C3S4Fig2">4.3</a> A) represents the dependencies of the simulated nonlinear system. From Fig. <a href="information-theory-and-statistical-causality.html#fig:C3S4Fig2">4.3</a> B) and Fig. <a href="information-theory-and-statistical-causality.html#fig:C3S4Fig2">4.3</a> C), we observe that the nonlinear interactions introduced were not captured by the linear form of the information flow. While all linear interactions presented similar linear and nonlinear information flows, the two nonlinear interactions introduced in the system presented relatively higher nonlinear information flow compared to the linear formulation.</p>
<div class="figure" style="text-align: center"><span id="fig:C3S4Fig2"></span>
<img src="chapters/TransferEntropy/S4Fig2.png" alt="Interactions between the variables of the simulated nonlinear system." width="100%" />
<p class="caption">
Figure 4.3: Interactions between the variables of the simulated nonlinear system.
</p>
</div>
</div>
<div id="information-flow-among-international-stock-market-indices" class="section level2">
<h2><span class="header-section-number">4.7</span> Information Flow among International Stock Market Indices</h2>
<p>The world’s financial markets form a complex, dynamic network in which individual markets interact with one another. This multitude of interactions can lead to highly significant and unexpected effects, and it is vital to understand precisely how various markets around the world influence one another <span class="citation">(Junior, Mullokandov, and Kenett <a href="#ref-junior2015dependency">2015</a>)</span>.</p>
<p>In this section, we use Transfer Entropy for the identification of dependency relations among international stock market indices. First, we select some of the <a href="https://finance.yahoo.com/world-indices/">major global indices</a> for our analysis, namely the S&amp;P 500, the FTSE 100, the DAX, the EURONEXT 100 and the IBOVESPA, which track the following markets, respectively, the US, the UK, Germany, Europe and Brazil. They are defined by the following tickers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tickers&lt;-<span class="kw">c</span>(<span class="st">&quot;^GSPC&quot;</span>, <span class="st">&quot;^FTSE&quot;</span>, <span class="st">&quot;^GDAXI&quot;</span>, <span class="st">&quot;^N100&quot;</span>, <span class="st">&quot;^BVSP&quot;</span>)</code></pre></div>
<p>Next, we will load log-returns of daily closing adjusted prices for the selected indices as follows (see Appendix <a href="datasets.html#dt-indices">B.1</a> for code used to generate this dataset):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(xts)
dataset&lt;-<span class="kw">as.xts</span>(<span class="kw">read.zoo</span>(<span class="st">&#39;./data/global_indices_returns.csv&#39;</span>,
                  <span class="dt">header=</span><span class="ot">TRUE</span>,
                  <span class="dt">index.column=</span><span class="dv">1</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>))
<span class="kw">head</span>(dataset)</code></pre></div>
<pre><code>##                        X.GSPC   X.FTSE  X.GDAXI   X.N100   X.BVSP
## 2000-01-02 19:00:00  0.000000       NA  0.00000  0.00000  0.00000
## 2000-01-03 19:00:00 -0.039099  0.00000 -0.02456 -0.04179 -0.06585
## 2000-01-04 19:00:00  0.001920 -0.01969 -0.01297 -0.02726  0.02455
## 2000-01-05 19:00:00  0.000955 -0.01366 -0.00418 -0.00842 -0.00853
## 2000-01-06 19:00:00  0.026730  0.00889  0.04618  0.02296  0.01246
## 2000-01-09 19:00:00  0.011128  0.01570  0.02109  0.01716  0.04279</code></pre>
<p>The influence that one market plays in another is dynamic. Here, we will consider the time period from 01/01/2014 until today and we will omit days with invalid returns due to bad data using the function <code>NARV.omit</code> from the package <strong>IDPmisc</strong> as follows:</p>
<p>We will calculate pairwise Transfer Entropy among all indices considered and construct a matrix such that each value in the position <span class="math inline">\((i,j)\)</span> will contain the value Transfer Entropy from <span class="math inline">\(tickers[i]\)</span> to <span class="math inline">\(tickers[j]\)</span> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(RTransferEntropy)
<span class="kw">library</span>(future)

## Allow for parallel computing
<span class="kw">plan</span>(multiprocess)</code></pre></div>
<pre><code>## Warning: [ONE-TIME WARNING] Forked processing (&#39;multicore&#39;) is disabled
## in future (&gt;= 1.13.0) when running R from RStudio, because it is
## considered unstable. Because of this, plan(&quot;multicore&quot;) will fall
## back to plan(&quot;sequential&quot;), and plan(&quot;multiprocess&quot;) will fall back to
## plan(&quot;multisession&quot;) - not plan(&quot;multicore&quot;) as in the past. For more
## details, how to control forked processing or not, and how to silence this
## warning in future R sessions, see ?future::supportsMulticore</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate pairwise Transfer Entropy among global indices</span>
TE.matrix&lt;-<span class="kw">FApply.Pairwise</span>(dataset.post.crisis, <span class="kw">calc_ete</span>())
<span class="kw">rownames</span>(TE.matrix)&lt;-<span class="kw">colnames</span>(TE.matrix)&lt;-tickers</code></pre></div>
<p>The function <code>FApply.Pairwise</code> is an auxiliary function that simply applies a given function <code>D.Func</code> to all possible pairs of columns from a given matrix <code>X</code> as follows:</p>
Fig. <a href="information-theory-and-statistical-causality.html#fig:TEmatrix">4.4</a> displays the resulting Transfer Entropy matrix. We normalize the Transfer Entropy values by dividing it by the maximum value in the matrix such that all values range from 0 to 1. We observe that the highest information flow is from the US market to the UK market (^GSPC -&gt; ^FTSE). The second highest information flow is from the UK market to the US market. That’s a result we would expect as the US and the UK markets are strongly coupled historically.
<div class="figure" style="text-align: center"><span id="fig:TEmatrix"></span>
<img src="open-quant-live-book_files/figure-html/TEmatrix-1.png" alt="Normalized Transfer Entropy among international stock market indices." width="100%" />
<p class="caption">
Figure 4.4: Normalized Transfer Entropy among international stock market indices.
</p>
</div>
<p>We also calculate the marginal contribution of each market to the total Transfer Entropy in the system by calculating the sum of Transfer Entropy for each row in the Transfer Entropy matrix, which we also normalize such that all values range from 0 to 1:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TE.marginal&lt;-<span class="kw">apply</span>(TE.matrix, <span class="dv">1</span>, sum)
TE.marginal.norm&lt;-TE.marginal<span class="op">/</span><span class="kw">sum</span>(TE.marginal)
<span class="kw">print</span>(TE.marginal.norm)</code></pre></div>
<pre><code>##  ^GSPC  ^FTSE ^GDAXI  ^N100  ^BVSP 
##  0.272  0.211  0.193  0.155  0.170</code></pre>
<p>We observe that the US is the most influential market in the time period studied detaining 27.164% of the total Transfer Entropy followed by the UK and Germany with 21.059% and 19.276%, respectively. Japan and Brazil are the least influential markets with normalized Transfer Entropies of 15.549% and 16.952%, respectively.</p>
<p>An experiment left to the reader is to build a daily trading strategy that exploits information flow among international markets. The proposed thesis is that one could build a profitable strategy by placing bets on futures of market indices that receive significant information flow from markets that observed unexpected returns/movements.</p>
<p>For an extended analysis with a broader set of indices see <span class="citation">(Junior, Mullokandov, and Kenett <a href="#ref-junior2015dependency">2015</a>)</span>. The authors develop networks of international stock market indices using an information theoretical framework. They use 83 stock market indices of a diversity of countries, as well as their single day lagged values, to probe the correlation and the flow of information from one stock index to another taking into account different operating hours. They find that Transfer Entropy is an effective way to quantify the flow of information between indices, and that a high degree of information flow between indices lagged by one day coincides to same day correlation between them.</p>
</div>
<div id="other-applications" class="section level2">
<h2><span class="header-section-number">4.8</span> Other Applications</h2>
<div id="quantifying-information-flow-between-social-media-and-the-stock-market" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Quantifying Information Flow Between Social Media and the Stock Market</h3>
<p>Investors’ decisions are modulated not only by companies’ fundamentals but also by personal beliefs, peers influence and information generated from news and the Internet. Rational and irrational investor’s behavior and their relation with the market efficiency hypothesis <span class="citation">(Fama <a href="#ref-JOFI:JOFI518">1970</a>)</span> have been largely debated in the economics and financial literature <span class="citation">(Shleifer <a href="#ref-shleifer2000inefficient">2000</a>)</span>. However, it was only recently that the availability of vast amounts of data from online systems paved the way for the large-scale investigation of investor’s collective behavior in financial markets.</p>
<p>A research paper <span class="citation">(Souza and Aste <a href="#ref-2016arXiv160104535S">2016</a>)</span> used some of the methods studied in this Chapter to uncover that information flows from social media to stock markets revealing that tweets are causing market movements through a nonlinear complex interaction. The authors provide empirical evidence that suggests social media and stock markets have a nonlinear causal relationship. They take advantage of an extensive data set composed of social media messages related to DJIA index components. By using information-theoretic measures to cope for possible nonlinear causal effects between social media and the stock market, the work points out stunning differences in the results with respect to linear coupling. Two main conclusions are drawn: First, social media significant causality on stocks’ returns are purely nonlinear in most cases; Second, social media dominates the directional coupling with stock market, an effect not observable within linear modeling. Results also serve as empirical guidance on model adequacy in the investigation of sociotechnical and financial systems.</p>
<p>Fig. <a href="information-theory-and-statistical-causality.html#fig:sigpoints-0">4.5</a> shows the significant causality links found between social media and stocks’ returns considering both cases: nonlinear (Transfer Entropy) and linear G-causality (linear VAR framework). The linear analysis discovers only three stocks with significant causality: INTEL CORP., NIKE INC. and WALT DISNEY CO. The Nonlinear analysis discovers that several other stocks have significant causality. In addition to the 3 stocks identified with significant linear causality, other 8 stocks presented purely nonlinear causality.</p>
<div class="figure" style="text-align: center"><span id="fig:sigpoints-0"></span>
<img src="chapters/TransferEntropy/SM-R-33.png" alt="Demonstration that the causality between social media and stocks' returns are mostly nonlinear. Linear causality test indicated that social media caused stock's returns only for 3 stocks. Nonparametric analysis showed that almost 1/3 of the stocks rejected in the linear case have significant nonlinear causality. In the nonlinear case, Transfer Entropy was used to quantify causal inference between the systems with randomized permutations test for significance estimation. In the linear case, a standard linear G-causality test was performed with a F-test under a linear vector-autoregressive framework. A significant linear G-causality was accepted if its linear specification was not rejected by the BDS test. p-values are adjusted with the Bonferroni correction. Significance is given at p-value  &lt; 0.05." width="100%" />
<p class="caption">
Figure 4.5: Demonstration that the causality between social media and stocks’ returns are mostly nonlinear. Linear causality test indicated that social media caused stock’s returns only for 3 stocks. Nonparametric analysis showed that almost 1/3 of the stocks rejected in the linear case have significant nonlinear causality. In the nonlinear case, Transfer Entropy was used to quantify causal inference between the systems with randomized permutations test for significance estimation. In the linear case, a standard linear G-causality test was performed with a F-test under a linear vector-autoregressive framework. A significant linear G-causality was accepted if its linear specification was not rejected by the BDS test. p-values are adjusted with the Bonferroni correction. Significance is given at p-value &lt; 0.05.
</p>
</div>
<p>The low level of causality obtained under linear constraints is in-line with results from similar studies in the literature, where it was found that stocks’ returns show weak causality links <span class="citation">(Alanyali, Moat, and Preis <a href="#ref-Tobias:2013">2013</a>, <span class="citation">Antweiler and Frank (<a href="#ref-Antweiler+Frank:04a">2004</a>)</span>)</span> and social media sentiment analytics, at least when taken alone, have small or no predictive power <span class="citation">(Ranco <a href="#ref-10.1371/journal.pone.0138441">2015</a>)</span> and do not have significant lead-time information about stock’s movements for the majority of the stocks <span class="citation">(Zheludev, Smith, and Aste <a href="#ref-citeulike:13108056">2014</a>)</span>. Contrariwise, results from the nonlinear analyses unveiled a much higher level of causality indicating that linear constraints may be neglecting the relationship between social media and stock markets.</p>
<p>In summary, this paper <span class="citation">(Souza and Aste <a href="#ref-2016arXiv160104535S">2016</a>)</span> is a good example on how causality can be not only complex but also misleading further highlighting the importance of choice in the methodology used to quantify it.</p>
</div>
<div id="detecting-causal-links-between-investor-sentiment-and-cryptocurrency-prices" class="section level3">
<h3><span class="header-section-number">4.8.2</span> Detecting Causal Links Between Investor Sentiment and Cryptocurrency Prices</h3>
<p>In <span class="citation">(Keskin and Aste <a href="#ref-keskin2019information">2019</a>)</span>, the authors use information-theoretic measures studied in this Chapter for non-linear causality detection applied to social media sentiment and cryptocurrency prices.</p>
<p>Using these techniques on sentiment and price data over a 48-month period to August 2018, for four major cryptocurrencies, namely bitcoin (BTC), ripple (XRP), litecoin (LTC) and ethereum (ETH), the authors detect significant information transfer, on hourly timescales, in directions of both sentiment to price and of price to sentiment. The work reports the scale of non-linear causality to be an order of magnitude greater than linear causality.</p>
<p>The information-theoretic investigation detected a significant non-linear causal relationship in BTC, LTC and XRP, over multiple timescales and in both the directions sentiment to price and price to sentiment. The effect was strongest and most consistent for BTC and LTC. Fig. <a href="information-theory-and-statistical-causality.html#fig:te-crypto">4.6</a> shows Transfer Entropy results between BTC sentiment and BTC prices.</p>
<div class="figure" style="text-align: center"><span id="fig:te-crypto"></span>
<img src="chapters/TransferEntropy/te-crypto.png" alt="Evidence that BTC sentiment and price are causally coupled in both directions in a non-linear way. Non-linear TE is calculated by multidimensional histograms with 6 quantile bins per dimension. Z-scores, calculated over 50 shuffles, show a high level of significance, especially during 2017 and 2018, in both directions." width="100%" />
<p class="caption">
Figure 4.6: Evidence that BTC sentiment and price are causally coupled in both directions in a non-linear way. Non-linear TE is calculated by multidimensional histograms with 6 quantile bins per dimension. Z-scores, calculated over 50 shuffles, show a high level of significance, especially during 2017 and 2018, in both directions.
</p>
</div>
<p>All analysis for this paper was performed using a Python package (PyCausality), which is available at <a href="https://github.com/ZacKeskin/PyCausality" class="uri">https://github.com/ZacKeskin/PyCausality</a>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Wiener56">
<p>Wiener, N. 1956. “The theory of prediction.” In <em>Modern Mathematics for Engineers</em>, edited by E. F. Beckenbach. McGraw-Hill, New York.</p>
</div>
<div id="ref-granger:econ">
<p>Granger, Clive. 1969. “Investigating Causal Relations by Econometric Models and Cross-Spectral Methods.” <em>Econometrica</em> 37 (3): 424–38.</p>
</div>
<div id="ref-PhysRevLett.85.461">
<p>Schreiber, Thomas. 2000. “Measuring Information Transfer.” <em>Phys. Rev. Lett.</em> 85 (2). American Physical Society: 461–64. doi:<a href="https://doi.org/10.1103/PhysRevLett.85.461">10.1103/PhysRevLett.85.461</a>.</p>
</div>
<div id="ref-citeulike:1447442">
<p>Hlavackovaschindler, K., M. Palus, M. Vejmelka, and J. Bhattacharya. 2007. “Causality Detection Based on Information-Theoretic Approaches in Time Series Analysis.” <em>Physics Reports</em> 441 (1): 1–46. doi:<a href="https://doi.org/10.1016/j.physrep.2006.12.004">10.1016/j.physrep.2006.12.004</a>.</p>
</div>
<div id="ref-10.1371/journal.pone.0109462">
<p>Montalto, Luca AND Marinazzo, Alessandro AND Faes. 2014. “MuTE: A Matlab Toolbox to Compare Established and Novel Estimators of the Multivariate Transfer Entropy.” <em>PLoS ONE</em> 9 (10). Public Library of Science: e109462. doi:<a href="https://doi.org/10.1371/journal.pone.0109462">10.1371/journal.pone.0109462</a>.</p>
</div>
<div id="ref-Michalowicz:2013:HDE:2601840">
<p>Michalowicz, Joseph Victor, Jonathan M. Nichols, and Frank Bucholtz. 2013. <em>Handbook of Differential Entropy</em>. Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-PhysRevLett.103.238701">
<p>Barnett, Lionel, Adam B. Barrett, and Anil K. Seth. 2009. “Granger Causality and Transfer Entropy Are Equivalent for Gaussian Variables.” <em>Phys. Rev. Lett.</em> 103 (23). American Physical Society: 238701. doi:<a href="https://doi.org/10.1103/PhysRevLett.103.238701">10.1103/PhysRevLett.103.238701</a>.</p>
</div>
<div id="ref-junior2015dependency">
<p>Junior, Leonidas, Asher Mullokandov, and Dror Kenett. 2015. “Dependency Relations Among International Stock Market Indices.” <em>Journal of Risk and Financial Management</em> 8 (2). Multidisciplinary Digital Publishing Institute: 227–65.</p>
</div>
<div id="ref-JOFI:JOFI518">
<p>Fama, E. F. 1970. “Efficient Capital Markets: A Review of Theory and Empirical Work.” <em>The Journal of Finance</em> 25 (2). Blackwell Publishing Ltd: 383–417. doi:<a href="https://doi.org/10.1111/j.1540-6261.1970.tb00518.x">10.1111/j.1540-6261.1970.tb00518.x</a>.</p>
</div>
<div id="ref-shleifer2000inefficient">
<p>Shleifer, A. 2000. <em>Inefficient Markets: An Introduction to Behavioral Finance</em>. Clarendon Lectures in Economics. OUP Oxford.</p>
</div>
<div id="ref-2016arXiv160104535S">
<p>Souza, T. T. P., and T. Aste. 2016. “A nonlinear impact: evidences of causal effects of social media on market prices.” <em>ArXiv E-Prints. Http://Arxiv.org/Abs/1601.04535</em>, January. arXiv preprint. http://arxiv.org/abs/1601.04535.</p>
</div>
<div id="ref-Tobias:2013">
<p>Alanyali, M., H. S. Moat, and T. Preis. 2013. “Quantifying the Relationship Between Financial News and the Stock Market.” <em>Sci. Rep.</em> 3.</p>
</div>
<div id="ref-Antweiler+Frank:04a">
<p>Antweiler, Werner, and Murray Z. Frank. 2004. “Is All That Talk Just Noise? The Information Content of Internet Stock Message Boards.” <em>Journal of Finance</em> 59 (3): 1259–94.</p>
</div>
<div id="ref-10.1371/journal.pone.0138441">
<p>Ranco, D. AND Caldarelli, G. AND Aleksovski. 2015. “The Effects of Twitter Sentiment on Stock Price Returns.” <em>PLoS ONE</em> 10 (9). Public Library of Science: e0138441. doi:<a href="https://doi.org/10.1371/journal.pone.0138441">10.1371/journal.pone.0138441</a>.</p>
</div>
<div id="ref-citeulike:13108056">
<p>Zheludev, I., R. Smith, and T. Aste. 2014. “When Can Social Media Lead Financial Markets?” <em>Scientific Reports</em> 4 (February).</p>
</div>
<div id="ref-keskin2019information">
<p>Keskin, Z, and T Aste. 2019. “Information-Theoretic Measures for Non-Linear Causality Detection: Application to Social Media Sentiment and Cryptocurrency Prices.” <em>arXiv Preprint arXiv:1906.05740</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="entropy.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="financial-networks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/souzatharsis/open-quant-live-book/edit/master/chapters/TransferEntropy/TransferEntropy.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["open-quant-live-book.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
