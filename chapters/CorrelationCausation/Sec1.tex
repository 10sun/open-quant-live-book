
We quantify causality by using the notion of the causal relation introduced by Granger where a signal $X$ is said to Granger-cause $Y$ if the future realizations of $Y$ can be better explained using the past information from $X$ and $Y$ rather than $Y$ alone.

The most common definitions of Granger-causality rely on the prediction of a future
value of the variable $Y$ by using the past values of $X$ and $Y$ itself. %use of two prediction functions $f_1(.)$ and $f_2(.)$ and the measure of the prediction goodness.
In that form, $X$ is said to G-cause $Y$ if the use of $X$ improves the prediction of $Y$.

Let $X_t$ be a random variable associated at time $t$ while $X^t$ represents the collection of random variables up to time $t$.
We consider ${X_t}, {Y_t}$ and ${Z_t}$ to be three stochastic processes.
Let $\hat Y_{t+1}$ be a predictor for the value of the variable $Y$ at time $t+1$.

We compare the expected value of a loss function $g(e)$ with the error $e=\hat{Y}_{t+1} - Y_{t+1}$ of two models:
\begin{itemize}
\item[1)] The expected value of the prediction error given only $Y^t$
\begin{equation}
 \mathcal{R}(Y^{t+1} \, | \, Y^t,Z^t) = \mathbb{E}[g(Y_{t+1} - f_1(X^{t},Z^t))]
\end{equation}
\item[2)] The expected value of the prediction error given $Y^t$ and $X^t$
\begin{equation}
 \mathcal{R}(Y^{t+1} \, | \, X^{t},Y^t,Z^t) = \mathbb{E}[g(Y_{t+1} - f_2(X^{t},Y^t,Z^t))].
\end{equation}
\end{itemize}
In both models, the functions $f_1(.)$ and $f_2(.)$ are chosen to minimize the expected value of the loss function.  In most cases, these functions are retrieved with linear and, possibly, with nonlinear regressions.
Typical forms for $g(.)$ are the $l1$- or $l2$-norms.
